---

- name: install the latest version of unzip
  yum: name=unzip state=latest

- name: remove local training directory and files
  file: path={{ item }} state=absent
  with_items:
    - "{{ training_dir }}"
    - 'streamrock.zip'


- name: create local training directories
  file: path={{ item }} state=directory mode=0755
  with_items:
    - "{{ training_dir }}"
    - "{{ training_jar_dir }}"
    - "{{ training_data_dir }}"
    - "{{ training_data_lyrics_dir }}"


- name: download avro tools
  get_url: url=http://central.maven.org/maven2/org/apache/avro/avro-tools/1.7.7/avro-tools-1.7.7.jar dest={{ training_jar_dir }} mode=0755


- name: download streamrock zip
  command: wget https://www.dropbox.com/s/u5tlk8kts005qim/streamrock.zip


- name: download and unarchive streamrock.zip
  unarchive: src=streamrock.zip dest={{ training_dir }} copy=no

- name: Copy scripts
  copy: src="{{ item }}" dest=/tmp mode=755
  with_items:
    - create-core-hive-tables.sh
    - concerts.tsv
    - onboard_concerts.sh
    - generate_lyrics_index.py


- name: remove hdfs directory
  command: hdfs dfs -rm -r -f {{ training_data_dir }} {{ training_jar_dir }}
  become: yes
  become_user: hdfs


- name: create hdfs directories
  command: hdfs dfs -mkdir -p {{ item }}
  become: yes
  become_user: hdfs
  with_items:
    - "{{ training_data_dir }}/user"
    - "{{ training_data_dir }}/streamsimple"
    - "{{ training_data_dir }}/stream"
    - "{{ training_data_dir }}/stream-2014-01-01"
    - "{{ training_data_dir }}/track"
    - "{{ training_data_dir }}/wordhappiness"
    - "{{ training_data_dir }}/playedWithMetadata"
    - "{{ training_data_dir }}/flight"
    - "{{ training_data_dir }}/airport"
    - "{{ training_data_dir }}/lyrics"
    - "{{ training_data_dir }}/concert"
    - "{{ training_data_dir }}/audit"
    - "/incoming/logs/upload"
    - "/incoming/mysql/concert"
    - "/user/hive/warehouse"

- name: chmod incoming directories
  command: hdfs dfs -chmod -R 777 /incoming
  become: yes
  become_user: hdfs

- name: chmod /user/hive/warehouse
  command: hdfs dfs -chmod 777 /user/hive/warehouse

- name: upload data to trainig data dir in HDFS
  command: hdfs dfs -put {{ training_data_dir }}/{{ item.file }} {{ training_data_dir }}/{{ item.dir }}
  become: yes
  become_user: hdfs
  with_items:
    - { file: 'users.tsv', dir: 'user' }
    - { file: 'played.tsv', dir: 'streamsimple' }
    - { file: 'stream.tsv', dir: 'stream' }
    - { file: 'stream.tsv.2014-01-01', dir: 'stream-2014-01-01' }
    - { file: 'song.tsv', dir: 'track' }
    - { file: 'wordhappiness.tsv', dir: 'wordhappiness' }
    - { file: 'playedWithMetadata.tsv', dir: 'playedWithMetadata' } 
    - { file: 'lyrics.avro', dir: 'lyrics' }
    - { file: 'lyrics.avro', dir: 'audit' }
    - { file: 'user.parquet', dir: '.' }
    - { file: 'concerts.json', dir: 'concert' }

- name: changing ownership for audit/lyrics.avro
  command: hdfs dfs -chown hdfs:bird {{ training_data_dir }}/audit/lyrics.avro
  become: yes
  become_user: hdfs

- name: changing permissions for audit/lyrics.avro
  command: hdfs dfs -chmod 640 {{ training_data_dir }}/audit/lyrics.avro
  become: yes
  become_user: hdfs

- name: upload data to training jar dir in HDFS
  command: hdfs dfs -put {{ training_jar_dir }} {{ training_jar_dir }}
  become: yes
  become_user: hdfs


- name: permissions for Camus jobs
  command: hdfs dfs -chmod 777 /
  become: yes
  become_user: hdfs


- name: Hive directory
  file: path=/var/lib/hive owner=hive group=hive mode=0777 state=directory


- name: recreate Hive tables
  command: /tmp/create-core-hive-tables.sh {{ training_data_dir }}


- name: Create lyrics for a user
  command: /tmp/generate_lyrics_index.py {{ item }} {{ training_data_dir }}/song.tsv {{ training_data_dir }}/lyrics_index.json {{ training_data_lyrics_dir }}
  with_items:
    - "{{ users }}"


- name: Onboard streamrock and concert
  command: /tmp/onboard_concerts.sh
